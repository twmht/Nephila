åˆç¨±ç‚º Cross Attention

```python
import torch
import torch.nn.functional as F

class EncoderDecoderAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        super(EncoderDecoderAttention, self).__init__()
        self.embed_dim = embed_dim

        # Q ä¾†è‡ª Decoderï¼ŒK å’Œ V ä¾†è‡ª Encoder
        self.W_Q = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_K = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = torch.nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, encoder_output, decoder_input):
        # ç”¢ç”Ÿ Q, K, V
        Q = self.W_Q(decoder_input)  # (batch, tgt_seq_len, embed_dim)
        K = self.W_K(encoder_output)  # (batch, src_seq_len, embed_dim)
        V = self.W_V(encoder_output)  # (batch, src_seq_len, embed_dim)

        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        d_k = self.embed_dim ** 0.5
        scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k

        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        attention_weights = F.softmax(scores, dim=-1)

        # è¨ˆç®—åŠ æ¬Šè¼¸å‡º
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights


# æ¨¡æ“¬ Encoder è¼¸å‡º (batch=1, src_seq_len=5, embed_dim=4)
encoder_output = torch.rand(1, 5, 4)

# æ¨¡æ“¬ Decoder ç›®å‰è¼¸å…¥ (batch=1, tgt_seq_len=3, embed_dim=4)
decoder_input = torch.rand(1, 3, 4)

# å»ºç«‹ Encoder-Decoder Attention æ¨¡å‹
attn = EncoderDecoderAttention(embed_dim=4)

# åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶
output, attn_weights = attn(encoder_output, decoder_input)

print("Attention Output:\n", output)
print("\nAttention Weights:\n", attn_weights)

```

å…¶ä¸­ attention_output çš„ shape æ˜¯ (batch_size, target_sequence_length, embedding_dimension)

### **Attention Output çš„ shape æ˜¯ `(1,3,4)`ï¼Œå®ƒçš„ç›´è§€æ„ç¾©æ˜¯ä»€éº¼ï¼Ÿ**

åœ¨ `Encoder-Decoder Attention` ä¸­ï¼Œé€™å€‹ shape `(1,3,4)` ä»£è¡¨ï¼š

```
(batch_size, target_sequence_length, embedding_dimension)
```

- `1` ğŸ‘‰ **batch size**ï¼Œä»£è¡¨é€™æ˜¯è™•ç†ä¸€å€‹æ¨£æœ¬ï¼ˆå¥å­ï¼‰ã€‚
- `3` ğŸ‘‰ **target sequence length**ï¼Œä»£è¡¨ **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆäº† 3 å€‹è©**ï¼Œé€™æ˜¯ `Decoder` ç›®å‰çš„è¼¸å‡ºé•·åº¦ã€‚
- `4` ğŸ‘‰ **embedding dimension**ï¼Œä»£è¡¨ **æ¯å€‹è©çš„å‘é‡ç¶­åº¦ç‚º 4**ï¼Œé€™æ˜¯ Transformer å…§éƒ¨çš„è¡¨ç¤ºç©ºé–“ã€‚

---

## **å…·é«”å«ç¾©ï¼š**

é€™å€‹ Attention Output `(1,3,4)` **æ˜¯ä¸€çµ„æ–°çš„è©åµŒå…¥è¡¨ç¤ºï¼ˆcontextual embeddingsï¼‰**ï¼Œå®ƒå€‘å·²ç¶“åŒ…å«äº† **ä¾†è‡ª Encoder çš„è³‡è¨Š**ã€‚

- **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆ 3 å€‹è©**ï¼ˆä¾‹å¦‚ `"Le chat est"`ï¼‰ã€‚
- **é€™ 3 å€‹è©çš„æ¯å€‹è©å‘é‡**ï¼Œæ˜¯é€šé **Encoder çš„è¼¸å‡ºè©** è¨ˆç®— **æ³¨æ„åŠ›åŠ æ¬Šæ±‚å’Œ** å¾Œç²å¾—çš„ã€‚
- **é€™äº›è©å‘é‡æœƒè¢«é€å…¥ Decoder çš„ä¸‹ä¸€å±¤ï¼ˆå¦‚å‰é¥‹ç¥ç¶“ç¶²çµ¡ï¼ŒFeed Forward Networkï¼‰ä¾†ç”Ÿæˆæœ€çµ‚çš„è¼¸å‡ºè©**ã€‚

---

## **èˆ‰ä¾‹èªªæ˜**

å‡è¨­æˆ‘å€‘è¦ç¿»è­¯ï¼š

```text
The cat sits on the mat.  (ä¾†æºèªè¨€ - English)
```

å°æ‡‰çš„ Decoder **ç›®å‰ç”Ÿæˆçš„è©** ç‚ºï¼š

```text
Le chat est  (ç›®æ¨™èªè¨€ - French)
```

é‚£éº¼ `Attention Output` çš„ shape `(1,3,4)`ï¼š

- `3` ğŸ‘‰ ä»£è¡¨ `"Le"`, `"chat"`, `"est"` é€™ 3 å€‹è©ã€‚
- `4` ğŸ‘‰ ä»£è¡¨å®ƒå€‘çš„å‘é‡ï¼Œæ¯å€‹è©çš„éš±è—å±¤è¡¨ç¤ºï¼ˆhidden representationï¼‰ã€‚

é€™äº›è©å‘é‡ **å·²ç¶“åŒ…å«äº†ä¾†è‡ª Encoder çš„è³‡è¨Š**ï¼Œä¾‹å¦‚ï¼š

- `"chat"`ï¼ˆè²“ï¼‰ å¯èƒ½ä¸»è¦é—œæ³¨ `"cat"`ã€‚
- `"est"`ï¼ˆæ˜¯ï¼‰ å¯èƒ½ä¸»è¦é—œæ³¨ `"sits"`ã€‚

é€™äº›è©å‘é‡æœƒé€²ä¸€æ­¥ç”¨æ–¼ç”Ÿæˆ `"assis"`ã€`"sur"`ã€`"le"`ã€`"tapis"` ç­‰è©ï¼Œç›´åˆ°å®Œæ•´ç”Ÿæˆ `"Le chat est assis sur le tapis."`ã€‚

---

## **ç¸½çµ**

`Attention Output` çš„å½¢ç‹€ `(1,3,4)`ï¼š

1. **3** ğŸ‘‰ ä»£è¡¨ **Decoder ç›®å‰ç”Ÿæˆäº† 3 å€‹è©**ã€‚
2. **4** ğŸ‘‰ ä»£è¡¨ **æ¯å€‹è©çš„åµŒå…¥ç¶­åº¦**ï¼Œé€™äº›å‘é‡ç”¨ä¾†å­¸ç¿’èªè¨€çš„èªç¾©é—œä¿‚ã€‚
3. **é€™äº›è©çš„å‘é‡å·²ç¶“è€ƒæ…®äº† Encoder çš„è³‡è¨Š**ï¼Œæ˜¯æœ€çµ‚ç”¢ç”Ÿæ­£ç¢ºè¼¸å‡ºçš„é—œéµã€‚

ğŸš€ **é€™å°±æ˜¯ Encoder-Decoder Attention å¦‚ä½•å¹«åŠ© Transformer æ¨¡å‹ç†è§£ä¸¦ç¿»è­¯å¥å­çš„æ–¹å¼ï¼**

### **Attention Weights çš„ shape æ˜¯ `(1,3,5)`ï¼Œå®ƒçš„ç›´è§€æ„ç¾©æ˜¯ä»€éº¼ï¼Ÿ**

é€™å€‹ shape `(1,3,5)` ä»£è¡¨ï¼š

```
(batch_size, target_sequence_length, source_sequence_length)
```

- **`1`** ğŸ‘‰ **batch size**ï¼Œè¡¨ç¤ºé€™æ˜¯ä¸€å€‹æ¨£æœ¬ï¼ˆå¥å­ï¼‰ã€‚
- **`3`** ğŸ‘‰ **target sequence length**ï¼Œè¡¨ç¤º **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆäº† 3 å€‹è©**ï¼ˆå³ `"Le"`, `"chat"`, `"est"`ï¼‰ã€‚
- **`5`** ğŸ‘‰ **source sequence length**ï¼Œè¡¨ç¤º **Encoder è¼¸å…¥å¥å­æœ‰ 5 å€‹è©**ï¼ˆå³ `"The"`, `"cat"`, `"sits"`, `"on"`, `"mat"`ï¼‰ã€‚

---

## **å…·é«”å«ç¾©ï¼š**

é€™å€‹ `Attention Weights (1,3,5)` **æ˜¯ä¸€å€‹æ³¨æ„åŠ›çŸ©é™£**ï¼Œè¡¨ç¤º **Decoder åœ¨ç”Ÿæˆæ¯å€‹è©æ™‚ï¼Œå®ƒå° Encoder è¼¸å‡ºçš„é—œæ³¨ç¨‹åº¦**ã€‚

æ›å¥è©±èªªï¼š

- æ¯ä¸€å€‹ `target token`ï¼ˆDecoder ç›®å‰å·²ç”Ÿæˆçš„è©ï¼‰**æœƒé—œæ³¨ Encoder çš„è¼¸å…¥è©**ï¼Œç”¢ç”Ÿä¸€çµ„æ©Ÿç‡åˆ†å¸ƒï¼ˆåŠ ç¸½ç‚º 1ï¼‰ã€‚
- é€™å€‹ `5` è¡¨ç¤º **Decoder ç”Ÿæˆçš„æ¯å€‹è©éƒ½æœƒæŸ¥çœ‹ Encoder è¼¸å‡ºçš„ 5 å€‹è©ï¼Œä¸¦åˆ†é…æ¬Šé‡**ã€‚
- é€™äº› **æ³¨æ„åŠ›æ¬Šé‡ï¼ˆattention scoresï¼‰** æœƒå‘Šè¨´æ¨¡å‹æ‡‰è©²ä¸»è¦é—œæ³¨å“ªäº›è¼¸å…¥è©ä¾†ç”Ÿæˆæ­£ç¢ºçš„ç¿»è­¯ã€‚

---

## **èˆ‰ä¾‹èªªæ˜**

å‡è¨­æˆ‘å€‘è¦ç¿»è­¯ï¼š

```text
Source Sentence (Encoder è¼¸å…¥) : "The cat sits on the mat."
Target Sentence (Decoder ç›®å‰è¼¸å‡º) : "Le chat est"
```

é‚£éº¼ **Attention Weights (1,3,5) çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€å€‹ Decoder ç”Ÿæˆè©çš„æ¬Šé‡**ï¼š

|Decoder ç”Ÿæˆè©|`"The"`|`"cat"`|`"sits"`|`"on"`|`"mat"`|
|---|---|---|---|---|---|
|`"Le"`|0.1|0.5|0.2|0.1|0.1|
|`"chat"`|0.05|0.7|0.2|0.03|0.02|
|`"est"`|0.05|0.1|0.75|0.05|0.05|

- `"Le"` ä¸»è¦é—œæ³¨ `"cat"`ï¼ˆå› ç‚º `"Le chat"` æ˜¯å° `"The cat"` çš„ç¿»è­¯ï¼‰ã€‚
- `"chat"` ä¹Ÿä¸»è¦é—œæ³¨ `"cat"`ï¼Œä½†å¯èƒ½ç¨å¾®çœ‹äº†ä¸€ä¸‹ `"sits"`ã€‚
- `"est"` ä¸»è¦é—œæ³¨ `"sits"`ï¼ˆå› ç‚º `"est"` æ˜¯ `"sits"` çš„å°æ‡‰è©ï¼‰ã€‚

é€™äº›æ¬Šé‡ç¢ºä¿äº† **æ¯å€‹è¼¸å‡ºè©åœ¨ç¿»è­¯æ™‚ï¼Œåªé—œæ³¨å°æ‡‰çš„é‡è¦è¼¸å…¥è©**ã€‚

---

## **ç‚ºä»€éº¼ Attention Weights æ˜¯ `(target_seq_len, source_seq_len)`ï¼Ÿ**

é€™æ˜¯å› ç‚ºï¼š

1. **æ¯å€‹ Decoder ç”Ÿæˆçš„è©éƒ½éœ€è¦è¨ˆç®—èˆ‡ Encoder è¼¸å…¥è©çš„ç›¸ä¼¼åº¦**ã€‚
2. **Softmax ä¿è­‰æ¯å€‹ Decoder ç”Ÿæˆè©çš„æ¬Šé‡ç¸½å’Œç‚º 1**ï¼ˆç¢ºä¿æ˜¯ä¸€å€‹æœ‰æ•ˆçš„æ©Ÿç‡åˆ†å¸ƒï¼‰ã€‚
3. **é€™äº›æ¬Šé‡æ±ºå®šäº†è¼¸å‡ºçš„è©æ‡‰è©²å¦‚ä½•çµ„åˆ Encoder çš„è³‡è¨Š**ã€‚

---

## **ç¸½çµ**

ğŸ“Œ `Attention Weights` çš„ shape `(1,3,5)` ä»£è¡¨ï¼š

- **3** ğŸ‘‰ **Decoder ç›®å‰ç”Ÿæˆäº† 3 å€‹è©**ã€‚
- **5** ğŸ‘‰ **Encoder è¼¸å…¥åŒ…å« 5 å€‹è©ï¼Œæ¯å€‹è©éƒ½æœ‰ä¸åŒçš„é‡è¦ç¨‹åº¦**ã€‚
- **é€™äº›æ¬Šé‡æ±ºå®š Decoder ç”Ÿæˆæ¯å€‹è©æ™‚æ‡‰è©²é—œæ³¨ Encoder çš„å“ªäº›è©**ï¼Œå¾è€Œæé«˜ç¿»è­¯æº–ç¢ºæ€§ã€‚

ğŸš€ **é€™å°±æ˜¯ Transformer æ¨¡å‹å¦‚ä½•åˆ©ç”¨ Encoder-Decoder Attention ä¾†é€²è¡Œé«˜å“è³ªç¿»è­¯çš„é—œéµï¼**