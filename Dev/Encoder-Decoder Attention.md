åˆç¨±ç‚º Cross Attention
## **Encoder-Decoder Attention è§£é‡‹**

**Encoder-Decoder Attention**ï¼ˆç·¨ç¢¼å™¨-è§£ç¢¼å™¨æ³¨æ„åŠ›ï¼‰æ˜¯ Transformer æ¨¡å‹åœ¨è™•ç†åºåˆ—å°ï¼ˆå¦‚ç¿»è­¯ã€æ‘˜è¦ç”Ÿæˆç­‰ï¼‰æ™‚çš„ä¸€ç¨® **äº¤å‰æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆCross-Attentionï¼‰**ã€‚å®ƒçš„ä½œç”¨æ˜¯è®“è§£ç¢¼å™¨ï¼ˆDecoderï¼‰åœ¨æ¯ä¸€æ­¥ç”Ÿæˆè©èªæ™‚ï¼Œæ ¹æ“š **ç·¨ç¢¼å™¨ï¼ˆEncoderï¼‰æä¾›çš„ä¸Šä¸‹æ–‡è³‡è¨Šä¾†æ±ºå®šæ‡‰è©²é—œæ³¨å“ªäº›è¼¸å…¥è©**ã€‚

---

## **1. Encoder-Decoder Attention èˆ‡ Self-Attention çš„å€åˆ¥**

åœ¨ Transformer å…§éƒ¨ï¼Œæœ‰ **ä¸‰ç¨®ä¸åŒçš„ Attention**ï¼š

1. **Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼šç™¼ç”Ÿåœ¨ Encoder å’Œ Decoder å…§éƒ¨ï¼Œæ¯å€‹ token èƒ½å¤ é—œæ³¨è‡ªå·±å’Œå…¶ä»– tokenã€‚
2. **Masked Self-Attentionï¼ˆæ©ç¢¼è‡ªæ³¨æ„åŠ›ï¼‰**ï¼šç™¼ç”Ÿåœ¨ Decoderï¼Œé¿å…æœªä¾†è©çš„è³‡è¨Šæ´©éœ²ã€‚
3. **Encoder-Decoder Attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰**ï¼šç™¼ç”Ÿåœ¨ Decoderï¼Œå…è¨± Decoder è®€å– Encoder çš„è¼¸å‡ºè³‡è¨Šã€‚

| **æ³¨æ„åŠ›é¡å‹**                     | **Query ä¾†æº** | **Key ä¾†æº** | **Value ä¾†æº** | **æ‡‰ç”¨å ´æ™¯**        |
| ----------------------------- | ------------ | ---------- | ------------ | --------------- |
| **Self-Attention**            | ä¾†è‡ªè¼¸å…¥è‡ªèº«       | ä¾†è‡ªè¼¸å…¥è‡ªèº«     | ä¾†è‡ªè¼¸å…¥è‡ªèº«       | Encoderã€Decoder |
| **Masked Self-Attention**     | ä¾†è‡ªè§£ç¢¼å™¨        | ä¾†è‡ªè§£ç¢¼å™¨      | ä¾†è‡ªè§£ç¢¼å™¨        | Decoderï¼ˆé¿å…è³‡è¨Šæ´©éœ²ï¼‰ |
| **Encoder-Decoder Attention** | ä¾†è‡ªè§£ç¢¼å™¨        | ä¾†è‡ªç·¨ç¢¼å™¨      | ä¾†è‡ªç·¨ç¢¼å™¨        | Decoderï¼ˆé—œæ³¨è¼¸å…¥ï¼‰   |

---

## **2. Encoder-Decoder Attention è¨ˆç®—æ­¥é©Ÿ**

å‡è¨­æˆ‘å€‘è¦æŠŠè‹±æ–‡å¥å­ï¼š

> "The cat sits on the mat."

ç¿»è­¯æˆæ³•èªï¼š

> "Le chat est assis sur le tapis."

é€™æ™‚å€™ï¼Œ**Encoder è² è²¬è™•ç†è¼¸å…¥å¥å­**ï¼Œ**Decoder é€æ­¥ç”Ÿæˆè¼¸å‡ºå¥å­**ï¼Œè€Œ Encoder-Decoder Attention è®“ Decoder å¯ä»¥å°ˆæ³¨æ–¼ Encoder çš„é‡è¦è³‡è¨Šã€‚

### **Step 1: Encoder ç”Ÿæˆ Keyï¼ˆKï¼‰å’Œ Valueï¼ˆVï¼‰**

è¼¸å…¥åºåˆ— `"The cat sits on the mat."` æœƒé€²é **Self-Attention** å’Œ **å‰é¥‹ç¥ç¶“ç¶²çµ¡ï¼ˆFeed Forward Networkï¼‰** è½‰æ›æˆ **éš±è—è¡¨ç¤º** HHï¼š

H=Encoder(X)H = \text{Encoder}(X)

é€™äº›ç·¨ç¢¼å¾Œçš„éš±è—å‘é‡ HH æœƒä½œç‚º **Keyï¼ˆKï¼‰å’Œ Valueï¼ˆVï¼‰**ï¼š

K=HWK,V=HWVK = H W_K, \quad V = H W_V

é€™è¡¨ç¤º **Encoder æŠŠè¼¸å…¥å¥å­è®Šæˆä¸€å€‹æ›´è±å¯Œçš„èªç¾©è¡¨ç¤º**ï¼Œä¸¦æº–å‚™çµ¦ Decoder ä½¿ç”¨ã€‚

---

### **Step 2: Decoder ç”Ÿæˆ Queryï¼ˆQï¼‰**

åœ¨è§£ç¢¼éç¨‹ä¸­ï¼ŒDecoder æœƒæ ¹æ“š **å·²ç”Ÿæˆçš„éƒ¨åˆ†** ä¾†æ±ºå®šä¸‹ä¸€å€‹è©ï¼š

Q=DWQQ = D W_Q

å…¶ä¸­ï¼ŒDD æ˜¯ Decoder ç›®å‰å·²ç¶“ç”¢ç”Ÿçš„è©ï¼Œä¾‹å¦‚åœ¨ç¿»è­¯æ™‚ï¼ŒDecoder å¯èƒ½å·²ç¶“ç”Ÿæˆ `"Le chat"`ï¼Œå®ƒéœ€è¦æŸ¥è©¢ **åœ¨ Encoder çš„è¼¸å‡ºä¸­å“ªäº›è©é‡è¦**ã€‚

---

### **Step 3: è¨ˆç®— Attention Score**

Decoder ä½¿ç”¨ Queryï¼ˆQï¼‰èˆ‡ Encoder çš„ Keyï¼ˆKï¼‰è¨ˆç®— **é—œè¯åº¦**ï¼š

AttentionÂ Score=QKTdk\text{Attention Score} = \frac{Q K^T}{\sqrt{d_k}}

é€™è¡¨ç¤º **Decoder ç›®å‰çš„è©æ‡‰è©²é—œæ³¨ Encoder ç”¢ç”Ÿçš„å“ªäº›è©**ã€‚ä¾‹å¦‚ï¼Œç•¶ Decoder å·²ç¶“ç”Ÿæˆ `"Le chat"`ï¼Œé‚£éº¼å®ƒå¯èƒ½æœƒé—œæ³¨ Encoder è¼¸å‡ºçš„ `"cat"`ã€‚

---

### **Step 4: è¨ˆç®—æ¬Šé‡ä¸¦å–å¾—åŠ æ¬Šè¼¸å‡º**

å°‡ **Attention Score** ç¶“é **Softmax** å¾—åˆ°æ¬Šé‡ï¼š

AttentionÂ Weights=Softmax(AttentionÂ Score)\text{Attention Weights} = \text{Softmax}(\text{Attention Score})

æ¥è‘—ç”¨é€™äº›æ¬Šé‡å° **Encoder çš„ Valueï¼ˆVï¼‰** é€²è¡ŒåŠ æ¬Šæ±‚å’Œï¼š

AttentionÂ Output=AttentionÂ WeightsÃ—V\text{Attention Output} = \text{Attention Weights} \times V

é€™æ¨£ï¼ŒDecoder å°±èƒ½æ ¹æ“š **Encoder è¼¸å‡ºçš„é—œéµè³‡è¨Šä¾†ç”Ÿæˆä¸‹ä¸€å€‹è©**ã€‚

---

## **3. Encoder-Decoder Attention çš„ä½œç”¨**

4. **è®“ Decoder å–å¾— Encoder è¼¸å‡ºçš„é‡è¦è³‡è¨Š**
    
    - åœ¨æ©Ÿå™¨ç¿»è­¯ä¸­ï¼Œæ¯å€‹ç”Ÿæˆè©æ™‚ï¼ŒDecoder éœ€è¦æ ¹æ“šè¼¸å…¥èªå¥çš„ä¸åŒéƒ¨åˆ†ä¾†æ±ºå®šè¦ç”Ÿæˆå“ªå€‹è©ã€‚
    - ä¾‹å¦‚ï¼Œåœ¨ç¿»è­¯ `"The cat sits on the mat."` æ™‚ï¼Œç•¶ Decoder ç”Ÿæˆ `"Le chat"`ï¼Œå®ƒæ‡‰è©²é—œæ³¨ `"The cat"`ï¼Œä½†ç•¶ç”Ÿæˆ `"assis"ï¼ˆåè‘—ï¼‰"` æ™‚ï¼Œå®ƒæ‡‰è©²é—œæ³¨ `"sits"`ã€‚
5. **æ”¹å–„é•·è·é›¢ä¾è³´å•é¡Œ**
    
    - å‚³çµ±çš„ Seq2Seqï¼ˆLSTM-basedï¼‰æ–¹æ³•å¾ˆé›£è¨˜ä½é•·å¥å­é–‹é ­çš„è³‡è¨Šï¼Œä½† Self-Attention èƒ½è®“ Decoder ç›´æ¥é—œæ³¨æ•´å€‹è¼¸å…¥å¥å­ï¼Œæé«˜ç¿»è­¯è³ªé‡ã€‚
6. **å…è¨±ä¸¦è¡Œè¨ˆç®—**
    
    - å‚³çµ± LSTM éœ€è¦é€æ­¥è§£ç¢¼ï¼Œè€Œ **Self-Attention å’Œ Encoder-Decoder Attention å¯ä»¥ä¸¦è¡Œè¨ˆç®—**ï¼ŒåŠ é€Ÿæ¨ç†ã€‚

---

## **4. Encoder-Decoder Attention çš„ PyTorch å¯¦ç¾**

```python
import torch
import torch.nn.functional as F

class EncoderDecoderAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        super(EncoderDecoderAttention, self).__init__()
        self.embed_dim = embed_dim

        # Q ä¾†è‡ª Decoderï¼ŒK å’Œ V ä¾†è‡ª Encoder
        self.W_Q = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_K = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = torch.nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, encoder_output, decoder_input):
        # ç”¢ç”Ÿ Q, K, V
        Q = self.W_Q(decoder_input)  # (batch, tgt_seq_len, embed_dim)
        K = self.W_K(encoder_output)  # (batch, src_seq_len, embed_dim)
        V = self.W_V(encoder_output)  # (batch, src_seq_len, embed_dim)

        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        d_k = self.embed_dim ** 0.5
        scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k

        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        attention_weights = F.softmax(scores, dim=-1)

        # è¨ˆç®—åŠ æ¬Šè¼¸å‡º
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
```

### **ç¯„ä¾‹è¼¸å…¥**

```python
# æ¨¡æ“¬ Encoder è¼¸å‡º (batch=1, src_seq_len=5, embed_dim=4)
encoder_output = torch.rand(1, 5, 4)

# æ¨¡æ“¬ Decoder ç›®å‰è¼¸å…¥ (batch=1, tgt_seq_len=3, embed_dim=4)
decoder_input = torch.rand(1, 3, 4)

# å»ºç«‹ Encoder-Decoder Attention æ¨¡å‹
attn = EncoderDecoderAttention(embed_dim=4)

# åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶
output, attn_weights = attn(encoder_output, decoder_input)

print("Attention Output:\n", output)
print("\nAttention Weights:\n", attn_weights)
```

---

## **5. å°çµ**

- **Encoder-Decoder Attention æ˜¯ Transformer å…§éƒ¨çš„ã€Œç¿»è­¯æ©‹æ¨‘ã€**ï¼Œè®“ **Decoder èƒ½å¤ æ ¹æ“š Encoder çš„è¼¸å‡ºé¸æ“‡é—œéµè³‡è¨Š**ã€‚
- **Query ä¾†è‡ª Decoderï¼ŒKey å’Œ Value ä¾†è‡ª Encoder**ï¼Œè®“è§£ç¢¼å™¨èƒ½æ ¹æ“šåŸæ–‡ç”Ÿæˆæœ€ç›¸é—œçš„è©ã€‚
- **è¨ˆç®—æ–¹å¼èˆ‡ Self-Attention é¡ä¼¼**ï¼Œä½†è·¨è¶Šäº† Encoder å’Œ Decoderï¼Œå¯¦ç¾äº†åºåˆ—å°æ˜ å°„ï¼ˆå¦‚ç¿»è­¯ã€æ‘˜è¦ï¼‰ã€‚
- **é€™æ˜¯æ©Ÿå™¨ç¿»è­¯ï¼ˆå¦‚ Google ç¿»è­¯ï¼‰ã€èªéŸ³è½‰æ–‡å­—ï¼ˆWhisperï¼‰ã€æ–‡æœ¬ç”Ÿæˆï¼ˆChatGPTï¼‰ç­‰æ‡‰ç”¨çš„é—œéµæŠ€è¡“ï¼** ğŸš€

```python
import torch
import torch.nn.functional as F

class EncoderDecoderAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        super(EncoderDecoderAttention, self).__init__()
        self.embed_dim = embed_dim

        # Q ä¾†è‡ª Decoderï¼ŒK å’Œ V ä¾†è‡ª Encoder
        self.W_Q = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_K = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = torch.nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, encoder_output, decoder_input):
        # ç”¢ç”Ÿ Q, K, V
        Q = self.W_Q(decoder_input)  # (batch, tgt_seq_len, embed_dim)
        K = self.W_K(encoder_output)  # (batch, src_seq_len, embed_dim)
        V = self.W_V(encoder_output)  # (batch, src_seq_len, embed_dim)

        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        d_k = self.embed_dim ** 0.5
        scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k

        # è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡
        attention_weights = F.softmax(scores, dim=-1)

        # è¨ˆç®—åŠ æ¬Šè¼¸å‡º
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights


# æ¨¡æ“¬ Encoder è¼¸å‡º (batch=1, src_seq_len=5, embed_dim=4)
encoder_output = torch.rand(1, 5, 4)

# æ¨¡æ“¬ Decoder ç›®å‰è¼¸å…¥ (batch=1, tgt_seq_len=3, embed_dim=4)
decoder_input = torch.rand(1, 3, 4)

# å»ºç«‹ Encoder-Decoder Attention æ¨¡å‹
attn = EncoderDecoderAttention(embed_dim=4)

# åŸ·è¡Œæ³¨æ„åŠ›æ©Ÿåˆ¶
output, attn_weights = attn(encoder_output, decoder_input)

print("Attention Output:\n", output)
print("\nAttention Weights:\n", attn_weights)

```

å…¶ä¸­ attention_output çš„ shape æ˜¯ (batch_size, target_sequence_length, embedding_dimension)

### **Attention Output çš„ shape æ˜¯ `(1,3,4)`ï¼Œå®ƒçš„ç›´è§€æ„ç¾©æ˜¯ä»€éº¼ï¼Ÿ**

åœ¨ `Encoder-Decoder Attention` ä¸­ï¼Œé€™å€‹ shape `(1,3,4)` ä»£è¡¨ï¼š

```
(batch_size, target_sequence_length, embedding_dimension)
```

- `1` ğŸ‘‰ **batch size**ï¼Œä»£è¡¨é€™æ˜¯è™•ç†ä¸€å€‹æ¨£æœ¬ï¼ˆå¥å­ï¼‰ã€‚
- `3` ğŸ‘‰ **target sequence length**ï¼Œä»£è¡¨ **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆäº† 3 å€‹è©**ï¼Œé€™æ˜¯ `Decoder` ç›®å‰çš„è¼¸å‡ºé•·åº¦ã€‚
- `4` ğŸ‘‰ **embedding dimension**ï¼Œä»£è¡¨ **æ¯å€‹è©çš„å‘é‡ç¶­åº¦ç‚º 4**ï¼Œé€™æ˜¯ Transformer å…§éƒ¨çš„è¡¨ç¤ºç©ºé–“ã€‚

---

## **å…·é«”å«ç¾©ï¼š**

é€™å€‹ Attention Output `(1,3,4)` **æ˜¯ä¸€çµ„æ–°çš„è©åµŒå…¥è¡¨ç¤ºï¼ˆcontextual embeddingsï¼‰**ï¼Œå®ƒå€‘å·²ç¶“åŒ…å«äº† **ä¾†è‡ª Encoder çš„è³‡è¨Š**ã€‚

- **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆ 3 å€‹è©**ï¼ˆä¾‹å¦‚ `"Le chat est"`ï¼‰ã€‚
- **é€™ 3 å€‹è©çš„æ¯å€‹è©å‘é‡**ï¼Œæ˜¯é€šé **Encoder çš„è¼¸å‡ºè©** è¨ˆç®— **æ³¨æ„åŠ›åŠ æ¬Šæ±‚å’Œ** å¾Œç²å¾—çš„ã€‚
- **é€™äº›è©å‘é‡æœƒè¢«é€å…¥ Decoder çš„ä¸‹ä¸€å±¤ï¼ˆå¦‚å‰é¥‹ç¥ç¶“ç¶²çµ¡ï¼ŒFeed Forward Networkï¼‰ä¾†ç”Ÿæˆæœ€çµ‚çš„è¼¸å‡ºè©**ã€‚

---

## **èˆ‰ä¾‹èªªæ˜**

å‡è¨­æˆ‘å€‘è¦ç¿»è­¯ï¼š

```text
The cat sits on the mat.  (ä¾†æºèªè¨€ - English)
```

å°æ‡‰çš„ Decoder **ç›®å‰ç”Ÿæˆçš„è©** ç‚ºï¼š

```text
Le chat est  (ç›®æ¨™èªè¨€ - French)
```

é‚£éº¼ `Attention Output` çš„ shape `(1,3,4)`ï¼š

- `3` ğŸ‘‰ ä»£è¡¨ `"Le"`, `"chat"`, `"est"` é€™ 3 å€‹è©ã€‚
- `4` ğŸ‘‰ ä»£è¡¨å®ƒå€‘çš„å‘é‡ï¼Œæ¯å€‹è©çš„éš±è—å±¤è¡¨ç¤ºï¼ˆhidden representationï¼‰ã€‚

é€™äº›è©å‘é‡ **å·²ç¶“åŒ…å«äº†ä¾†è‡ª Encoder çš„è³‡è¨Š**ï¼Œä¾‹å¦‚ï¼š

- `"chat"`ï¼ˆè²“ï¼‰ å¯èƒ½ä¸»è¦é—œæ³¨ `"cat"`ã€‚
- `"est"`ï¼ˆæ˜¯ï¼‰ å¯èƒ½ä¸»è¦é—œæ³¨ `"sits"`ã€‚

é€™äº›è©å‘é‡æœƒé€²ä¸€æ­¥ç”¨æ–¼ç”Ÿæˆ `"assis"`ã€`"sur"`ã€`"le"`ã€`"tapis"` ç­‰è©ï¼Œç›´åˆ°å®Œæ•´ç”Ÿæˆ `"Le chat est assis sur le tapis."`ã€‚

---

## **ç¸½çµ**

`Attention Output` çš„å½¢ç‹€ `(1,3,4)`ï¼š

7. **3** ğŸ‘‰ ä»£è¡¨ **Decoder ç›®å‰ç”Ÿæˆäº† 3 å€‹è©**ã€‚
8. **4** ğŸ‘‰ ä»£è¡¨ **æ¯å€‹è©çš„åµŒå…¥ç¶­åº¦**ï¼Œé€™äº›å‘é‡ç”¨ä¾†å­¸ç¿’èªè¨€çš„èªç¾©é—œä¿‚ã€‚
9. **é€™äº›è©çš„å‘é‡å·²ç¶“è€ƒæ…®äº† Encoder çš„è³‡è¨Š**ï¼Œæ˜¯æœ€çµ‚ç”¢ç”Ÿæ­£ç¢ºè¼¸å‡ºçš„é—œéµã€‚

ğŸš€ **é€™å°±æ˜¯ Encoder-Decoder Attention å¦‚ä½•å¹«åŠ© Transformer æ¨¡å‹ç†è§£ä¸¦ç¿»è­¯å¥å­çš„æ–¹å¼ï¼**

### **Attention Weights çš„ shape æ˜¯ `(1,3,5)`ï¼Œå®ƒçš„ç›´è§€æ„ç¾©æ˜¯ä»€éº¼ï¼Ÿ**

é€™å€‹ shape `(1,3,5)` ä»£è¡¨ï¼š

```
(batch_size, target_sequence_length, source_sequence_length)
```

- **`1`** ğŸ‘‰ **batch size**ï¼Œè¡¨ç¤ºé€™æ˜¯ä¸€å€‹æ¨£æœ¬ï¼ˆå¥å­ï¼‰ã€‚
- **`3`** ğŸ‘‰ **target sequence length**ï¼Œè¡¨ç¤º **Decoder ç›®å‰å·²ç¶“ç”Ÿæˆäº† 3 å€‹è©**ï¼ˆå³ `"Le"`, `"chat"`, `"est"`ï¼‰ã€‚
- **`5`** ğŸ‘‰ **source sequence length**ï¼Œè¡¨ç¤º **Encoder è¼¸å…¥å¥å­æœ‰ 5 å€‹è©**ï¼ˆå³ `"The"`, `"cat"`, `"sits"`, `"on"`, `"mat"`ï¼‰ã€‚

---

## **å…·é«”å«ç¾©ï¼š**

é€™å€‹ `Attention Weights (1,3,5)` **æ˜¯ä¸€å€‹æ³¨æ„åŠ›çŸ©é™£**ï¼Œè¡¨ç¤º **Decoder åœ¨ç”Ÿæˆæ¯å€‹è©æ™‚ï¼Œå®ƒå° Encoder è¼¸å‡ºçš„é—œæ³¨ç¨‹åº¦**ã€‚

æ›å¥è©±èªªï¼š

- æ¯ä¸€å€‹ `target token`ï¼ˆDecoder ç›®å‰å·²ç”Ÿæˆçš„è©ï¼‰**æœƒé—œæ³¨ Encoder çš„è¼¸å…¥è©**ï¼Œç”¢ç”Ÿä¸€çµ„æ©Ÿç‡åˆ†å¸ƒï¼ˆåŠ ç¸½ç‚º 1ï¼‰ã€‚
- é€™å€‹ `5` è¡¨ç¤º **Decoder ç”Ÿæˆçš„æ¯å€‹è©éƒ½æœƒæŸ¥çœ‹ Encoder è¼¸å‡ºçš„ 5 å€‹è©ï¼Œä¸¦åˆ†é…æ¬Šé‡**ã€‚
- é€™äº› **æ³¨æ„åŠ›æ¬Šé‡ï¼ˆattention scoresï¼‰** æœƒå‘Šè¨´æ¨¡å‹æ‡‰è©²ä¸»è¦é—œæ³¨å“ªäº›è¼¸å…¥è©ä¾†ç”Ÿæˆæ­£ç¢ºçš„ç¿»è­¯ã€‚

---

## **èˆ‰ä¾‹èªªæ˜**

å‡è¨­æˆ‘å€‘è¦ç¿»è­¯ï¼š

```text
Source Sentence (Encoder è¼¸å…¥) : "The cat sits on the mat."
Target Sentence (Decoder ç›®å‰è¼¸å‡º) : "Le chat est"
```

é‚£éº¼ **Attention Weights (1,3,5) çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€å€‹ Decoder ç”Ÿæˆè©çš„æ¬Šé‡**ï¼š

|Decoder ç”Ÿæˆè©|`"The"`|`"cat"`|`"sits"`|`"on"`|`"mat"`|
|---|---|---|---|---|---|
|`"Le"`|0.1|0.5|0.2|0.1|0.1|
|`"chat"`|0.05|0.7|0.2|0.03|0.02|
|`"est"`|0.05|0.1|0.75|0.05|0.05|

- `"Le"` ä¸»è¦é—œæ³¨ `"cat"`ï¼ˆå› ç‚º `"Le chat"` æ˜¯å° `"The cat"` çš„ç¿»è­¯ï¼‰ã€‚
- `"chat"` ä¹Ÿä¸»è¦é—œæ³¨ `"cat"`ï¼Œä½†å¯èƒ½ç¨å¾®çœ‹äº†ä¸€ä¸‹ `"sits"`ã€‚
- `"est"` ä¸»è¦é—œæ³¨ `"sits"`ï¼ˆå› ç‚º `"est"` æ˜¯ `"sits"` çš„å°æ‡‰è©ï¼‰ã€‚

é€™äº›æ¬Šé‡ç¢ºä¿äº† **æ¯å€‹è¼¸å‡ºè©åœ¨ç¿»è­¯æ™‚ï¼Œåªé—œæ³¨å°æ‡‰çš„é‡è¦è¼¸å…¥è©**ã€‚

---

## **ç‚ºä»€éº¼ Attention Weights æ˜¯ `(target_seq_len, source_seq_len)`ï¼Ÿ**

é€™æ˜¯å› ç‚ºï¼š

10. **æ¯å€‹ Decoder ç”Ÿæˆçš„è©éƒ½éœ€è¦è¨ˆç®—èˆ‡ Encoder è¼¸å…¥è©çš„ç›¸ä¼¼åº¦**ã€‚
11. **Softmax ä¿è­‰æ¯å€‹ Decoder ç”Ÿæˆè©çš„æ¬Šé‡ç¸½å’Œç‚º 1**ï¼ˆç¢ºä¿æ˜¯ä¸€å€‹æœ‰æ•ˆçš„æ©Ÿç‡åˆ†å¸ƒï¼‰ã€‚
12. **é€™äº›æ¬Šé‡æ±ºå®šäº†è¼¸å‡ºçš„è©æ‡‰è©²å¦‚ä½•çµ„åˆ Encoder çš„è³‡è¨Š**ã€‚

---

## **ç¸½çµ**

ğŸ“Œ `Attention Weights` çš„ shape `(1,3,5)` ä»£è¡¨ï¼š

- **3** ğŸ‘‰ **Decoder ç›®å‰ç”Ÿæˆäº† 3 å€‹è©**ã€‚
- **5** ğŸ‘‰ **Encoder è¼¸å…¥åŒ…å« 5 å€‹è©ï¼Œæ¯å€‹è©éƒ½æœ‰ä¸åŒçš„é‡è¦ç¨‹åº¦**ã€‚
- **é€™äº›æ¬Šé‡æ±ºå®š Decoder ç”Ÿæˆæ¯å€‹è©æ™‚æ‡‰è©²é—œæ³¨ Encoder çš„å“ªäº›è©**ï¼Œå¾è€Œæé«˜ç¿»è­¯æº–ç¢ºæ€§ã€‚

ğŸš€ **é€™å°±æ˜¯ Transformer æ¨¡å‹å¦‚ä½•åˆ©ç”¨ Encoder-Decoder Attention ä¾†é€²è¡Œé«˜å“è³ªç¿»è­¯çš„é—œéµï¼**