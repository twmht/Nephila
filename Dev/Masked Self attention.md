## **Masked Self-Attention è§£é‡‹**

### **1. Masked Self-Attention æ˜¯ä»€éº¼ï¼Ÿ**

Masked Self-Attentionï¼ˆæ©ç¢¼è‡ªæ³¨æ„åŠ›ï¼‰æ˜¯ Transformer **Decoder** å…§éƒ¨çš„ä¸€ç¨® **è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼Œå…¶æ ¸å¿ƒç›®æ¨™æ˜¯ï¼š

> **åœ¨ç”Ÿæˆä¸‹ä¸€å€‹è©æ™‚ï¼Œåªèƒ½é—œæ³¨å·²ç¶“ç”¢ç”Ÿçš„è©ï¼Œè€Œä¸èƒ½çœ‹åˆ°æœªä¾†çš„è©ï¼Œä»¥é˜²æ­¢è³‡è¨Šæ´©æ¼ã€‚**

é€™èˆ‡ä¸€èˆ¬çš„ **Self-Attention** ä¸åŒï¼Œå› ç‚ºæ™®é€šçš„ Self-Attention å…è¨±æ¯å€‹è©é—œæ³¨æ•´å€‹è¼¸å…¥åºåˆ—ï¼Œè€Œ Masked Self-Attention æœƒåˆ»æ„é®è”½æœªä¾†çš„è³‡è¨Šã€‚

### **2. Masked Self-Attention ç”¨åœ¨å“ªè£¡ï¼Ÿ**

- **GPT / ChatGPT**ï¼ˆç´” Decoder-only æ¨¡å‹ï¼‰
- **Transformer Decoderï¼ˆå¦‚ Whisperã€T5ã€BARTï¼‰**
- **æ©Ÿå™¨ç¿»è­¯ã€èªè¨€æ¨¡å‹**ï¼ˆéœ€è¦é€æ­¥ç”Ÿæˆçš„ä»»å‹™ï¼‰

ç•¶æˆ‘å€‘åœ¨ **Decoder** å…§éƒ¨åŸ·è¡Œ Masked Self-Attention æ™‚ï¼Œæ¯å€‹è© **åªèƒ½è§€å¯Ÿå®ƒä¹‹å‰çš„è©**ï¼Œè€Œä¸èƒ½çœ‹åˆ°å®ƒä¹‹å¾Œçš„è©ã€‚

---

## **2. Masked Self-Attention è¨ˆç®—æ­¥é©Ÿ**

Masked Self-Attention çš„è¨ˆç®—éç¨‹èˆ‡æ™®é€š **Self-Attention** é¡ä¼¼ï¼Œä½† **åœ¨è¨ˆç®—æ™‚æœƒä½¿ç”¨ Mask ä¾†é®è”½æœªä¾†çš„è©**ã€‚

### **Step 1: è¨ˆç®— Q, K, V**

èˆ‡ä¸€èˆ¬ Self-Attention ç›¸åŒï¼Œæ¯å€‹è¼¸å…¥ token éƒ½æœƒç”¢ç”Ÿ **Queryï¼ˆQï¼‰**ã€**Keyï¼ˆKï¼‰** å’Œ **Valueï¼ˆVï¼‰**ï¼š

Q=XWQ,K=XWK,V=XWVQ = X W_Q, \quad K = X W_K, \quad V = X W_V

é€™è£¡ï¼š

- QQï¼ˆQueryï¼‰ï¼šç”¨ä¾†æŸ¥è©¢é—œè¯è©ã€‚
- KKï¼ˆKeyï¼‰ï¼šç”¨ä¾†åŒ¹é… Query ä»¥è¨ˆç®—æ¬Šé‡ã€‚
- VVï¼ˆValueï¼‰ï¼šåŒ…å«å¯¦éš›çš„è©å‘é‡è³‡è¨Šã€‚

---

### **Step 2: è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸**

èˆ‡æ™®é€š Attention ä¸€æ¨£ï¼Œæˆ‘å€‘è¨ˆç®— Query å’Œ Key çš„ç›¸ä¼¼åº¦ï¼š

AttentionÂ Score=QKTdk\text{Attention Score} = \frac{Q K^T}{\sqrt{d_k}}

é€™å€‹è¨ˆç®—æœƒç”¢ç”Ÿä¸€å€‹ **(seq_len, seq_len)** çš„æ–¹é™£ï¼Œè¡¨ç¤º **åºåˆ—ä¸­æ¯å€‹è©èˆ‡å…¶ä»–è©çš„ç›¸ä¼¼åº¦**ã€‚

**èˆ‰ä¾‹ï¼š** å‡è¨­ Decoder ç›®å‰è™•ç† `"I love Machine Learning"`ï¼ŒSelf-Attention è¨ˆç®—å‡ºçš„é—œæ³¨çŸ©é™£å¯èƒ½æ˜¯ï¼š

```
      I    love  Machine Learning
I     âœ”ï¸    âœ”ï¸    âœ”ï¸      âœ”ï¸
love  âœ”ï¸    âœ”ï¸    âœ”ï¸      âœ”ï¸
Machine âœ”ï¸  âœ”ï¸    âœ”ï¸      âœ”ï¸
Learning âœ”ï¸ âœ”ï¸    âœ”ï¸      âœ”ï¸
```

åœ¨ **ä¸€èˆ¬çš„ Self-Attention** ä¸­ï¼Œæ¯å€‹è©å¯ä»¥é—œæ³¨ä»»ä½•å…¶ä»–è©ï¼ˆé›™å‘æ³¨æ„åŠ›ï¼‰ã€‚

---

### **Step 3: åŠ å…¥ Maskï¼Œéš±è—æœªä¾†è©**

åœ¨ **Masked Self-Attention** ä¸­ï¼Œæˆ‘å€‘ä½¿ç”¨ **ä¸‹ä¸‰è§’ Maskï¼ˆLower Triangular Maskï¼‰**ï¼Œé®è”½ **æœªä¾†è©çš„æ³¨æ„åŠ›åˆ†æ•¸**ï¼š

```
      I    love  Machine Learning
I     âœ”ï¸    âŒ    âŒ      âŒ
love  âœ”ï¸    âœ”ï¸    âŒ      âŒ
Machine âœ”ï¸  âœ”ï¸    âœ”ï¸      âŒ
Learning âœ”ï¸ âœ”ï¸    âœ”ï¸      âœ”ï¸
```

é€™æ„å‘³è‘—ï¼š

- `"I"` åªèƒ½é—œæ³¨ `"I"`ã€‚
- `"love"` åªèƒ½é—œæ³¨ `"I", "love"`ã€‚
- `"Machine"` åªèƒ½é—œæ³¨ `"I", "love", "Machine"`ã€‚
- `"Learning"` åªèƒ½é—œæ³¨ `"I", "love", "Machine", "Learning"`ã€‚

å¯¦éš›å¯¦ç¾æ™‚ï¼Œé€™æ˜¯é€é **å°‡ Masked å€åŸŸè¨­ç‚º -âˆ** ä¾†è®“ Softmax è®Šæˆ 0ï¼š

MaskedÂ Score=[âœ”âˆ’âˆâˆ’âˆâˆ’âˆâœ”âœ”âˆ’âˆâˆ’âˆâœ”âœ”âœ”âˆ’âˆâœ”âœ”âœ”âœ”]\text{Masked Score} = \begin{bmatrix} âœ” & -\infty & -\infty & -\infty \\ âœ” & âœ” & -\infty & -\infty \\ âœ” & âœ” & âœ” & -\infty \\ âœ” & âœ” & âœ” & âœ” \end{bmatrix}

---

### **Step 4: Softmax è¨ˆç®—æ³¨æ„åŠ›æ¬Šé‡**

å° **Masked Score** å¥—ç”¨ **Softmax**ï¼Œå°‡ -âˆ è½‰æ›ç‚º 0ï¼š

AttentionÂ Weights=Softmax(MaskedÂ Score)\text{Attention Weights} = \text{Softmax}(\text{Masked Score})

é€™æ¨£ï¼Œæœªä¾†çš„è©å°±ç„¡æ³•å½±éŸ¿ç•¶å‰çš„è©ï¼Œç¢ºä¿ **Decoder åªèƒ½ä½¿ç”¨éå»è©ä¾†ç”Ÿæˆæ–°è©**ã€‚

---

### **Step 5: è¨ˆç®—è¼¸å‡º**

æœ€å¾Œï¼Œæˆ‘å€‘å°‡ **Attention Weights** ä¹˜ä¸Š Valueï¼ˆVï¼‰ï¼š

Output=AttentionÂ WeightsÃ—V\text{Output} = \text{Attention Weights} \times V

é€™æ¨£ï¼Œæ¯å€‹è©çš„æœ€çµ‚è¡¨ç¤º **åªä¾†è‡ªæ–¼å®ƒå·²ç¶“çœ‹åˆ°çš„è©**ï¼Œç¢ºä¿èªè¨€æ¨¡å‹æ˜¯ **è‡ªå›æ­¸ï¼ˆAuto-Regressiveï¼‰** çš„ã€‚

---

## **3. Masked Self-Attention çš„ä½œç”¨**

âœ… **é˜²æ­¢æœªä¾†è³‡è¨Šæ´©éœ²**

- åœ¨è¨“ç·´éç¨‹ä¸­ï¼Œç¢ºä¿æ¨¡å‹ **åªèƒ½ä½¿ç”¨å·²ç”Ÿæˆçš„è©** ä¾†é æ¸¬ä¸‹ä¸€å€‹è©ï¼Œé€™å°æ–¼èªè¨€æ¨¡å‹è‡³é—œé‡è¦ã€‚

âœ… **é©ç”¨æ–¼è‡ªå›æ­¸æ¨¡å‹ï¼ˆAuto-Regressive Modelsï¼‰**

- **GPT, Whisper, T5, BART** éƒ½ä½¿ç”¨ Masked Self-Attentionï¼Œç¢ºä¿æ¨¡å‹é€æ­¥ç”Ÿæˆè¼¸å‡ºï¼Œè€Œä¸æœƒä¸€æ¬¡æ€§ç”¢ç”Ÿå®Œæ•´åºåˆ—ã€‚

âœ… **æå‡æ–‡æœ¬ç”Ÿæˆçš„æµæš¢æ€§**

- è®“æ¨¡å‹èƒ½å¤ å­¸ç¿’ã€Œé€æ­¥ç”Ÿæˆã€çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§é æ¸¬æ‰€æœ‰è©ï¼Œé€™æ¨£å¯ä»¥æ›´éˆæ´»åœ°èª¿æ•´è©åºã€‚

---

## **4. PyTorch å¯¦ç¾ Masked Self-Attention**

```python
import torch
import torch.nn.functional as F

class MaskedSelfAttention(torch.nn.Module):
    def __init__(self, embed_dim):
        super(MaskedSelfAttention, self).__init__()
        self.embed_dim = embed_dim

        # Q, K, V çš„ç·šæ€§è®Šæ›
        self.W_Q = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_K = torch.nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_V = torch.nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, x):
        seq_len = x.shape[1]

        # è¨ˆç®— Q, K, V
        Q = self.W_Q(x)  # (batch, seq_len, embed_dim)
        K = self.W_K(x)  # (batch, seq_len, embed_dim)
        V = self.W_V(x)  # (batch, seq_len, embed_dim)

        # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_dim ** 0.5)  # (batch, seq_len, seq_len)

        # å‰µå»º Maskï¼ˆä¸‹ä¸‰è§’çŸ©é™£ï¼‰
        mask = torch.tril(torch.ones(seq_len, seq_len))  # (seq_len, seq_len)
        scores = scores.masked_fill(mask == 0, float('-inf'))  # æ©è“‹æœªä¾†è©

        # Softmax è¨ˆç®—æ¬Šé‡
        attention_weights = F.softmax(scores, dim=-1)

        # è¨ˆç®—åŠ æ¬Šè¼¸å‡º
        output = torch.matmul(attention_weights, V)  # (batch, seq_len, embed_dim)

        return output, attention_weights

# æ¸¬è©¦ Masked Self-Attention
embed_dim = 4
seq_len = 5
x = torch.rand(1, seq_len, embed_dim)  # éš¨æ©Ÿåˆå§‹åŒ–è¼¸å…¥

masked_attention = MaskedSelfAttention(embed_dim)
output, attn_weights = masked_attention(x)

print("Masked Attention Weights:\n", attn_weights)
```

---

## **5. çµè«–**

**Masked Self-Attention**ï¼š

1. **èˆ‡ä¸€èˆ¬ Self-Attention ç›¸åŒï¼Œä½†æœƒé®è”½æœªä¾†è©**ã€‚
2. **ç¢ºä¿æ¨¡å‹é€æ­¥ç”Ÿæˆè©ï¼Œä¸æœƒæå‰ã€Œå·çœ‹ã€æœªä¾†çš„è©**ã€‚
3. **å»£æ³›æ‡‰ç”¨æ–¼ GPTã€Whisperã€BARTã€T5 ç­‰æ¨¡å‹**ï¼Œé©ç”¨æ–¼ç¿»è­¯ã€èªéŸ³è½‰éŒ„ã€æ–‡æœ¬ç”Ÿæˆç­‰ä»»å‹™ã€‚

ğŸš€ **Masked Self-Attention æ˜¯èªè¨€æ¨¡å‹ä¸­ã€Œè‡ªå›æ­¸ç”Ÿæˆã€çš„é—œéµæŠ€è¡“ï¼Œç¢ºä¿äº† Transformer èƒ½å¤ é€æ­¥ç”¢ç”Ÿåˆç†çš„è¼¸å‡ºï¼** ğŸ¯


## **Masked Self-Attention vs. Auto-Regressive Modelï¼ˆè‡ªå›æ­¸æ¨¡å‹ï¼‰**

é€™å…©å€‹æ¦‚å¿µåœ¨ Transformer æ¨¡å‹ä¸­æ¯æ¯ç›¸é—œï¼Œå°¤å…¶æ˜¯åœ¨ **GPTã€Whisperã€æ©Ÿå™¨ç¿»è­¯ï¼ˆå¦‚ T5, BARTï¼‰ã€èªéŸ³è­˜åˆ¥ï¼ˆASRï¼‰** ç­‰æ¨¡å‹ä¸­å»£æ³›ä½¿ç”¨ã€‚è®“æˆ‘å€‘ç”¨ä¸€å€‹å…·é«”çš„ç¯„ä¾‹ä¾†ç†è§£å®ƒå€‘çš„å€åˆ¥èˆ‡è¯ç¹«ã€‚

---

## **1. Masked Self-Attentionï¼šç¢ºä¿åºåˆ—é€æ­¥ç”Ÿæˆ**

### **ğŸ”¹ æ¦‚å¿µ**

**Masked Self-Attention** æ˜¯ Transformer **Decoder** å…§éƒ¨çš„ä¸€ç¨® **è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼Œå®ƒçš„ä½œç”¨æ˜¯ï¼š

> **è®“æ¯å€‹ token åªèƒ½é—œæ³¨å®ƒã€Œä¹‹å‰ã€çš„ tokenï¼Œè€Œä¸èƒ½çœ‹åˆ°ã€Œæœªä¾†ã€çš„ tokenï¼Œé˜²æ­¢è³‡è¨Šæ´©æ¼ã€‚**

é€™èˆ‡æ™®é€šçš„ **Self-Attentionï¼ˆå…¨å±€é—œæ³¨ï¼‰** ä¸åŒï¼Œå› ç‚ºæ™®é€šçš„ Self-Attention å…è¨±æ¯å€‹è©é—œæ³¨æ•´å€‹è¼¸å…¥ï¼Œè€Œ **Masked Self-Attention æœƒåˆ»æ„é®è”½æœªä¾†çš„è©**ã€‚

### **ğŸ”¹ ç¯„ä¾‹**

å‡è¨­æˆ‘å€‘ä½¿ç”¨ Transformer ä¾†ç”Ÿæˆå¥å­ï¼š

```
"I love NLP"
```

ç•¶æˆ‘å€‘ä½¿ç”¨ **Masked Self-Attention** æ™‚ï¼š

- **"I"** åªèƒ½çœ‹åˆ°è‡ªå·±ï¼Œä¸èƒ½çœ‹åˆ° "love" å’Œ "NLP"ã€‚
- **"love"** å¯ä»¥çœ‹åˆ° "I" ä½†ä¸èƒ½çœ‹åˆ° "NLP"ã€‚
- **"NLP"** å¯ä»¥çœ‹åˆ° "I" å’Œ "love"ã€‚

é€™ç”¨ä¸€å€‹çŸ©é™£ä¾†è¡¨ç¤ºï¼š

```
      I    love   NLP
I     âœ”ï¸    âŒ    âŒ
love  âœ”ï¸    âœ”ï¸    âŒ
NLP   âœ”ï¸    âœ”ï¸    âœ”ï¸
```

ğŸ”¹ **æ•¸å­¸è¡¨ç¤ºï¼ˆä½¿ç”¨ Maskï¼‰**ï¼š

MaskedÂ Score=[âœ”âˆ’âˆâˆ’âˆâœ”âœ”âˆ’âˆâœ”âœ”âœ”]\text{Masked Score} = \begin{bmatrix} âœ” & -\infty & -\infty \\ âœ” & âœ” & -\infty \\ âœ” & âœ” & âœ” \end{bmatrix}

é€™æ¨£ï¼Œæ¨¡å‹åœ¨ **è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸ï¼ˆSoftmax å‰ï¼‰** æ™‚ï¼š

- **è¢« Mask é®è”½çš„éƒ¨åˆ†æœƒè®Šæˆ -âˆï¼ŒSoftmax å¾Œæ©Ÿç‡è®Šç‚º 0**ï¼Œç¢ºä¿è©²è© **ç„¡æ³•é—œæ³¨æœªä¾†çš„è©**ã€‚

---

## **2. Auto-Regressive Modelï¼ˆè‡ªå›æ­¸æ¨¡å‹ï¼‰ï¼šé€æ­¥ç”Ÿæˆåºåˆ—**

### **ğŸ”¹ æ¦‚å¿µ**

**Auto-Regressive Modelï¼ˆè‡ªå›æ­¸æ¨¡å‹ï¼‰** æ˜¯ä¸€ç¨®é€æ­¥ç”Ÿæˆåºåˆ—çš„æ–¹æ³•ï¼Œå®ƒçš„ä¸»è¦ç‰¹é»æ˜¯ï¼š

> **ç•¶å‰çš„è©åªèƒ½ä¾è³´ã€Œéå»ã€å·²ç”Ÿæˆçš„è©ï¼Œé€æ­¥ç”¢ç”Ÿä¸‹ä¸€å€‹è©ã€‚**

é€™èˆ‡ **Masked Self-Attention çš„æ¦‚å¿µå®Œå…¨ä¸€è‡´**ï¼Œä½†ä¸åŒçš„æ˜¯ï¼š

- **Masked Self-Attention æ˜¯ Transformer Decoder å…§éƒ¨çš„ä¸€ç¨®è¨ˆç®—æ–¹æ³•**ã€‚
- **Auto-Regressive Model æ˜¯ä¸€ç¨®ç”Ÿæˆæ–¹å¼ï¼Œé©ç”¨æ–¼ RNNã€LSTMã€Transformer ç­‰æ¨¡å‹**ã€‚

### **ğŸ”¹ ç¯„ä¾‹**

ç•¶æˆ‘å€‘ä½¿ç”¨ **è‡ªå›æ­¸æ–¹å¼** ä¾†ç”Ÿæˆ `"I love NLP"` æ™‚ï¼š

1. **ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆ "I"**
    
    ```
    [START] -> "I"
    ```
    
    - é€™æ™‚å€™ **æ¨¡å‹æ²’æœ‰æ­·å²è³‡è¨Š**ï¼Œå®Œå…¨ä¾è³´ `<START>` ä¾†é æ¸¬ "I"ã€‚
2. **ç¬¬äºŒæ­¥ï¼šæ ¹æ“š "I" ä¾†ç”Ÿæˆ "love"**
    
    ```
    [START] -> "I" -> "love"
    ```
    
    - é€™æ™‚å€™æ¨¡å‹åªèƒ½çœ‹åˆ° `"I"`ï¼Œæ ¹æ“š `"I"` ä¾†é¸æ“‡æœ€å¯èƒ½çš„ä¸‹ä¸€å€‹è©ã€‚
3. **ç¬¬ä¸‰æ­¥ï¼šæ ¹æ“š "I love" ä¾†ç”Ÿæˆ "NLP"**
    
    ```
    [START] -> "I" -> "love" -> "NLP"
    ```
    
    - é€™æ™‚å€™æ¨¡å‹åªèƒ½çœ‹åˆ° `"I love"`ï¼Œæ ¹æ“šå®ƒä¾†é¸æ“‡ `"NLP"`ã€‚

é€™å€‹éç¨‹ç¨±ç‚º **è‡ªå›æ­¸ï¼ˆAuto-Regressiveï¼‰ç”Ÿæˆ**ï¼Œå› ç‚ºï¼š âœ… **æ¯ä¸€æ­¥çš„è¼¸å…¥ä¾†è‡ªæ–¼ã€Œéå»å·²ç¶“ç”Ÿæˆçš„è©ã€**  
âœ… **ç„¡æ³•ä¸€æ¬¡æ€§çœ‹åˆ°æ•´å€‹åºåˆ—ï¼Œå¿…é ˆé€æ­¥ç”Ÿæˆ**

é€™èˆ‡ Masked Self-Attention ç›¸åŒ¹é…ï¼Œå› ç‚º Mask ç¢ºä¿äº†æ¨¡å‹åœ¨ Decoder è¨“ç·´æ™‚ **åªèƒ½çœ‹åˆ°å·²ç”Ÿæˆçš„è©ï¼Œè€Œä¸èƒ½çœ‹åˆ°æœªä¾†çš„è©**ã€‚

---

## **3. Masked Self-Attention vs. Auto-Regressive Model**

|**æ¦‚å¿µ**|**Masked Self-Attention**|**Auto-Regressive Model**|
|---|---|---|
|**å®šç¾©**|Transformer Decoder å…§éƒ¨çš„ä¸€ç¨®æ©Ÿåˆ¶ï¼Œç”¨æ–¼ Mask æœªä¾†è©|ä¸€ç¨®é€æ­¥ç”Ÿæˆçš„æ–¹å¼ï¼Œç¢ºä¿æ¨¡å‹åªèƒ½ä¾è³´å·²ç”Ÿæˆçš„è©|
|**ç›®çš„**|é˜²æ­¢è³‡è¨Šæ´©éœ²ï¼Œç¢ºä¿ç”Ÿæˆéç¨‹æ˜¯é€æ­¥çš„|é¿å…ä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´åºåˆ—ï¼Œè®“æ¨¡å‹å­¸ç¿’åˆç†çš„èªè¨€çµæ§‹|
|**æ‡‰ç”¨**|Transformerï¼ˆGPT, Whisper, T5, BARTï¼‰|RNN, LSTM, Transformerï¼ˆGPT, Whisperï¼‰|
|**æ˜¯å¦ Mask æœªä¾†è©ï¼Ÿ**|æ˜¯|æ˜¯ï¼ˆå› ç‚ºå®ƒæ˜¯é€æ­¥ç”Ÿæˆçš„ï¼‰|
|**æ˜¯å¦éœ€è¦éå»è¼¸å‡ºï¼Ÿ**|ä¸éœ€è¦ï¼ˆå¯ä»¥å¹³è¡Œè¨ˆç®—ï¼‰|éœ€è¦ï¼ˆå› ç‚ºä¸‹ä¸€æ­¥çš„è¼¸å…¥ä¾†è‡ªå‰ä¸€æ­¥çš„è¼¸å‡ºï¼‰|
|**é‹è¡Œæ–¹å¼**|è¨“ç·´æ™‚ Mask æ‰€æœ‰æœªä¾†è©ï¼Œè®“ Decoder åªèƒ½é—œæ³¨éå»è©|æ¨ç†æ™‚é€æ­¥è¼¸å…¥å‰ä¸€æ­¥è¼¸å‡ºï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´åºåˆ—|

ğŸš€ **çµè«–ï¼š**

- **Masked Self-Attention æ˜¯ Auto-Regressive Model åœ¨ Transformer å…§éƒ¨çš„ä¸€ç¨®è¨ˆç®—æ–¹å¼ï¼**
- **å®ƒç¢ºä¿ Transformer Decoder åªèƒ½çœ‹éå»çš„è©ï¼Œè€Œä¸èƒ½çœ‹æœªä¾†çš„è©ï¼Œå¾è€Œæ»¿è¶³ Auto-Regressive ç”Ÿæˆçš„è¦æ±‚ã€‚**

---

## **4. PyTorch ç¯„ä¾‹**

### **ğŸ”¹ Masked Self-Attentionï¼ˆå…§éƒ¨è¨ˆç®—æ–¹å¼ï¼‰**

```python
import torch
import torch.nn.functional as F

def masked_self_attention(x):
    seq_len = x.shape[1]

    # å‰µå»º Q, K, V
    Q = x  
    K = x  
    V = x  

    # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (x.shape[-1] ** 0.5)

    # å‰µå»º Maskï¼Œé®è”½æœªä¾†è©
    mask = torch.tril(torch.ones(seq_len, seq_len))  # (seq_len, seq_len)
    scores = scores.masked_fill(mask == 0, float('-inf'))  

    # Softmax è¨ˆç®—æ¬Šé‡
    attention_weights = F.softmax(scores, dim=-1)

    # è¨ˆç®—åŠ æ¬Šè¼¸å‡º
    output = torch.matmul(attention_weights, V)  

    return output, attention_weights

# æ¸¬è©¦ç¯„ä¾‹
x = torch.rand(1, 5, 4)  # (batch, seq_len, embed_dim)
output, attn_weights = masked_self_attention(x)
print("Masked Attention Weights:\n", attn_weights)
```

---

### **ğŸ”¹ Auto-Regressiveï¼ˆé€æ­¥ç”Ÿæˆï¼‰**

```python
import torch
import torch.nn.functional as F

# å‡è¨­ vocab_size = 10000, embed_dim = 4
class AutoRegressiveModel(torch.nn.Module):
    def __init__(self, embed_dim, vocab_size):
        super(AutoRegressiveModel, self).__init__()
        self.embed_dim = embed_dim
        self.output_layer = torch.nn.Linear(embed_dim, vocab_size)

    def forward(self, hidden_state):
        logits = self.output_layer(hidden_state)  # (batch, seq_len, vocab_size)
        probabilities = F.softmax(logits, dim=-1)
        next_word_index = torch.argmax(probabilities, dim=-1)
        return next_word_index

# æ¸¬è©¦é€æ­¥ç”Ÿæˆ
model = AutoRegressiveModel(embed_dim=4, vocab_size=10000)
hidden_state = torch.rand(1, 1, 4)  # åªçµ¦ 1 å€‹è©
next_word_index = model(hidden_state)
print("Next Word Index:", next_word_index)
```

---

## **5. ç¸½çµ**

ğŸ“Œ **Masked Self-Attention**ï¼šåœ¨ **Transformer Decoder å…§éƒ¨** ä½¿ç”¨ Mask ä¾†é¿å…æœªä¾†è³‡è¨Šæ´©éœ²ï¼Œç¢ºä¿è‡ªå›æ­¸ç‰¹æ€§ã€‚  
ğŸ“Œ **Auto-Regressive Model**ï¼šä¸€ç¨® **é€æ­¥ç”Ÿæˆçš„æ–¹å¼**ï¼Œä¿è­‰ä¸‹ä¸€å€‹è© **åªèƒ½ä¾è³´éå»è©**ï¼Œæ‡‰ç”¨æ–¼ GPTã€Whisper ç­‰æ¨¡å‹ã€‚

ğŸš€ **Masked Self-Attention æ˜¯ Auto-Regressive çš„é—œéµæŠ€è¡“ï¼Œè®“ Transformer èƒ½å¤ æ­£ç¢ºåœ°é€æ­¥ç”Ÿæˆåºåˆ—ï¼** ğŸ¯