---
mindmap-plugin: basic
---

# YODAS (YouTube-Oriented Dataset for Audio and Speech)

## 1. YODAS 數據集概述
- 大規模、多語音資料集
	- 超過 **50 萬小時**的語音數據
	- 包含 **100 多種語言**
	- 從 YouTube 收集
	- **公開釋出**，使用 Creative Commons 授權
	- 分為三個子集:
		- **手動轉錄子集 (Manual Subset)**
			- **86,400 小時** 音頻數據，附帶人工轉錄
			- 品質較高，但資料量相對較少
		- **自動轉錄子集 (Automatic Subset)**
			- **335,845 小時** 音頻數據，附帶自動轉錄
			- 資料量較大，但品質較低
		- **未標記子集 (Unlabeled Subset)**
			- **144,174 小時** 原始音頻數據，沒有轉錄
			- 可用於自監督學習
- **目標**: 解決業界大規模語音數據集公開不足的問題

## 2. 數據收集方法
- **目標**:
	- 影片內容必須使用 Creative Commons 授權
	- 影片必須具有自動或手動字幕
- **架構**: 三種客戶端和一個主節點
	- **關鍵字客戶端 (Keyword-based Client)**
		- 從多語言 Wikipedia 文章中提取關鍵字
		- 使用 YouTube 的過濾功能搜尋符合條件的影片
		- 使用 AJAX 動態爬取較低排名的影片
	- **頻道客戶端 (Channel-based Client)**
		- 從關鍵字搜尋到的影片中提取頻道資訊
		- 搜尋同一頻道的所有相關影片，擴大資料量
	- **下載客戶端 (Download Client)**
		- 下載音訊並轉為單聲道 24kHz 格式
		- 下載可用的字幕
		- 使用啟發式方法選擇正確的字幕和語言
	- **主節點 (Master Node)**
		- 管理資源狀態（未開始、處理中、已完成）
		- 防止不同客戶端同時下載相同資源

## 3. 數據分析
- **語音分析**
	- **手動子集**：平均影片時長較短，平均語句時長較長
	- **自動子集**：平均影片時長較長，平均語句時長較短
		- 自動字幕傾向將長語句切分成小片段
	- **語言分佈**:
		- 英語佔據主要份額，其次為西班牙語和俄語
		- 自動子集的語言種類少於手動子集
- **文字分析**
	- **書寫系統**：拉丁字母最常用，其次為西里爾字母
	- **手動子集**：語句的字元數較多，變異性較大
	- **自動子集**：語句的字元數較少，變異性較小
- **語音-文字對齊**
	- **原始數據可能包含雜訊**，手動或自動字幕可能不完全準確
	- 使用預訓練的聲學模型計算語句對齊分數
	- 手動子集對齊分數較高，自動子集有較多對齊不良的語句
	- 使用對齊分數過濾數據，設定閾值 (例如 2.0)

## 4. 實驗與結果
- **目標**: 使用 YODAS 數據集進行單語語音辨識任務
- **基線模型**: 基於**預訓練的 XLSR** 模型
	- 在 XLSR 基礎上添加線性層，並使用 **CTC 損失函數**進行優化
	- 使用 ESPnet 和 s3prl 工具準備模型
	- 子詞詞彙使用 SentencePiece 的 BPE 算法
	- 不進行語音增強
	- 使用 AdamW 優化器
	- 解碼使用貪婪算法，不使用語言模型
- **效能評估**: 使用**字元錯誤率 (CER)** 衡量
- **實驗結果**:
	- 前 15 大語言的 CER 在 6 到 15 之間
	- 匈牙利語表現最佳 (CER 6.2)，日語表現最差 (CER 14.7)
	- 詞彙量較大的語言 (如中文和日語) CER 較高
	- **手動子集訓練的模型效能明顯優於自動子集訓練的模型**
		- 主要差異在於**刪除錯誤 (Deletion Error)**
	- 對齊分數閾值設定對模型效能影響重大
	- 增加訓練數據量可以提升模型效能，但超過一定量後效果不明顯

## 5. 結論與未來方向
- YODAS 數據集是一個有價值的資源，可供語音研究社群使用
- 未來可進一步研究如何改進數據品質和模型效能

## 6. 核心概念延伸
- **大規模語音數據集**:
	- 數據集的規模對於訓練高效能的語音辨識模型至關重要
	- 大規模數據集能幫助模型學習更魯棒的語音表示
- **多語音處理**:
	- 多語音數據集可用於訓練跨語言的語音辨識模型
	- 多語音數據集能促進低資源語言的語音技術發展
- **預訓練模型 (XLSR)**:
	- 預訓練模型能夠學習通用的語音表示，並應用於不同任務
	- 微調預訓練模型能夠更快地訓練出高效能的模型
- **自監督學習**:
	- 自監督學習能夠利用未標記的數據來提升模型效能
	- 可以使用未標記子集訓練自監督模型 (如 HuBERT, wav2vec2)
- **語音-文字對齊**:
	- 準確的語音-文字對齊對於訓練高品質的語音辨識模型至關重要
	- 對齊錯誤會導致模型效能下降
	- **數據品質**:
	- 數據品質比數據量更重要，高品質的手動轉錄資料優於低品質的自動轉錄資料
	- 應重視數據過濾和品質控制
- **CTC 損失函數**:
	- CTC 損失函數適用於序列到序列的語音辨識任務
	- 它能解決語音和文字之間時間軸長度不一致的問題

## **主要概念總結**:
- **YODAS是一個大規模多語言的語音數據集，包含手動、自動和未標記三種子集，可供研究使用。**
- **數據收集使用關鍵字和頻道爬取方法，並有主節點監控。**
- **手動轉錄子集的數據品質高於自動轉錄子集，在語音辨識任務上表現更好。**
- **基線模型使用預訓練的XLSR模型，並使用CTC損失函數優化。**
- **對齊分數和訓練數據量對模型效能有重要影響。**

## 中文資料集
- 都是人工標註
- 看起來是切片段，每個音檔只有一句話