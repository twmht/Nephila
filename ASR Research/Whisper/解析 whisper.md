You're absolutely right! Whisper operates at **16 kHz audio sampling rate**, which means the audio input is resampled to 16,000 samples per second before being processed. Let me clarify how this aligns with the **n_audio_ctx** parameter:

### Whisper's Processing Context:

1. **16 kHz and Mel Frames**:
    
    - Whisper uses a mel-spectrogram representation of the audio, where each frame represents **10 milliseconds (ms)** of audio.
    - æ¯ä¸€ç§’æœ‰ 100 å€‹ mel frame
2. **n_audio_ctx** and Duration:
    
    - The `n_audio_ctx` parameter specifies the **number of mel frames** the model processes at once.
    - For example:
        - If `n_audio_ctx = 1500`, é‚£éº¼å°±æ˜¯ 15 ç§’
        - If `n_audio_ctx = 3000`, é‚£éº¼å°±æ˜¯ 30 ç§’
3. **16 kHz Sampling and Mel Spectrogram Conversion**:
    
    - å…¶å¯¦è·Ÿ n_audio_ctx æ²’é—œä¿‚ï¼Œåªæ˜¯ whisper æœƒå…ˆæŠŠéŸ³æª”åš 16kHz çš„å–æ¨£ï¼Œä¹‹å¾Œåœ¨ç”¨ mel æ¿¾æ³¢å™¨å–ä¸€æ¬¡
    - Whisper converts the 16 kHz raw audio into a mel-spectrogram, which is then tokenized for processing.

### Practical Implication of 16 kHz:

- The 16 kHz sampling ensures high audio resolution, capturing a broader frequency range (up to 8 kHz, per Nyquist theorem) while maintaining manageable computational costs.
- The **n_audio_ctx** controls the amount of this resampled and processed mel-spectrogram data the model considers.

Let me know if this clears things up! ğŸ˜Š


10 ms per frame is a **definition from the Mel-spectrogram representation**, not unique to Whisper. Let me break it down further for clarity:

### 10 ms Per Frame: A Mel-spectrogram Standard

- The **10 ms per frame** convention is widely used in signal processing and automatic speech recognition (ASR) systems, including Whisper.
- This means that for every **10 milliseconds of audio**, a single frame in the Mel-spectrogram is generated.

### Whisper Adopts the Standard

- Whisper follows this established standard when it processes audio input:
    1. The audio is resampled to **16 kHz** (if it isn't already).
    2. It computes the Mel-spectrogram, where **each frame corresponds to 10 ms** of the original audio signal.

### Why 10 ms?

The choice of 10 ms has practical advantages:

1. **Resolution vs. Efficiency**:
    - 10 ms is fine-grained enough to capture speech features effectively.
    - It balances computational efficiency with detailâ€”smaller windows (e.g., 5 ms) provide higher resolution but are more computationally expensive.
2. **Standardization**:
    - This duration aligns with many ASR systems and tools, making it easier to compare results and integrate models.

### Whisper's Use of Frames

- Whisper's `n_audio_ctx` defines **how many of these Mel-spectrogram frames** (each representing 10 ms) it processes in a single context.
- For example:
    - `n_audio_ctx = 1500` means **1500 frames**, or **1500 Ã— 10 ms = 15 seconds of audio**.
    - This parameter directly impacts the duration of audio the model can "see" at once.

So, the 10 ms per frame is indeed a feature of the Mel-spectrogram representation Whisper uses, but it's not something Whisper specifically invented. Whisper adopts this convention as part of its overall architecture.

Let me know if you'd like further clarification! ğŸ˜Š

Not quite. If you have a signal sampled at **16 kHz** with **16,000 samples**, and you process it through the Mel-spectrogram transformation, the resulting number of frames depends on the **window size**, **hop size**, and the **FFT parameters** used in the transformation.

### Key Parameters in Mel-Spectrogram Conversion

1. **Sampling rate**: 16,000 Hz (16 kHz).
2. **Window size (e.g., 25 ms)**: Determines how many samples are in one analysis window. At 16 kHz, a 25 ms window corresponds to: 16,000Ã—0.025=400â€‰samples.16,000 \times 0.025 = 400 \, \text{samples}.
3. **Hop size (e.g., 10 ms)**: Determines how much the window shifts for each frame. At 16 kHz, a 10 ms hop corresponds to: 16,000Ã—0.010=160â€‰samples.16,000 \times 0.010 = 160 \, \text{samples}.

Using these parameters:

- The number of **frames** can be calculated as: frames=âŒˆtotalÂ samplesâˆ’windowÂ sizehopÂ sizeâŒ‰+1\text{frames} = \left\lceil \frac{\text{total samples} - \text{window size}}{\text{hop size}} \right\rceil + 1 For 16,000 samples: frames=âŒˆ16,000âˆ’400160âŒ‰+1=100â€‰frames.\text{frames} = \left\lceil \frac{16,000 - 400}{160} \right\rceil + 1 = 100 \, \text{frames}.

4. **Mel bins (fixed at 80 for Whisper)**: Determines the number of frequency bands in the Mel-spectrogram.

### Output Tensor Shape

- **Input samples**: 16,000 samples of audio (1 second at 16 kHz).
- **Mel-spectrogram**:
    - Frames: **100 frames** (based on the hop size of 10 ms).
    - Mel bins: **80 bins**.

Thus, the resulting shape is:

(1,80,100)(1, 80, 100)

### Summary

If you have 16,000 samples of audio at 16 kHz, the Mel-spectrogram tensor shape is:

- **(1, 80, 100)**, not **(1, 80, 160)**.

If you assumed 160 frames, it would imply a different hop size (likely around **6.25 ms**, which is unusual). Let me know if you want to explore this further!

æ˜¯çš„ï¼Œ**hop size** å¯ä»¥é¡æ¯”ç‚º **stride** çš„æ¦‚å¿µï¼Œå®ƒè¡¨ç¤ºåœ¨æ¯æ¬¡çª—å£æ»‘å‹•æ™‚ç§»åŠ¨çš„æ ·æœ¬æ•°é‡ã€‚

åœ¨éŸ³é¢‘å¤„ç†ä¸­ï¼Œ**hop size** æ˜¯æŒ‡è®¡ç®—è¿ç»­å¸§çš„ Mel é¢‘è°±æˆ– STFT æ—¶ï¼Œä¸¤ä¸ªçª—å£ä¹‹é—´çš„è·ç¦»ï¼ˆä»¥æ ·æœ¬æ•°ä¸ºå•ä½ï¼‰ã€‚è¿™ä¸ªå€¼ç›´æ¥å†³å®šäº†å¸§çš„æ—¶é—´åˆ†è¾¨ç‡ã€‚

ä»¥ä¸‹æ˜¯å¯¹ **hop size** å’Œ **stride** çš„å¯¹æ¯”è§£é‡Šï¼š

### ç±»æ¯”äºå›¾åƒå¤„ç†ä¸­å·ç§¯çš„ stride

1. **å·ç§¯ä¸­çš„ stride**:
    
    - stride è¡¨ç¤ºåœ¨è¾“å…¥ä¸­æ¯æ¬¡ç§»åŠ¨å·ç§¯æ ¸æ—¶çš„æ­¥é•¿ã€‚
    - stride å†³å®šäº†è¾“å‡ºç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ï¼ˆå°çš„ stride ç”Ÿæˆæ›´å¯†é›†çš„ç‰¹å¾å›¾ï¼Œå¤§çš„ stride ç”Ÿæˆæ›´ç¨€ç–çš„ç‰¹å¾å›¾ï¼‰ã€‚
2. **éŸ³é¢‘å¤„ç†ä¸­ hop size**:
    
    - hop size å†³å®šäº†è¿ç»­å¸§ä¹‹é—´çš„æ—¶é—´é—´éš”ã€‚
    - å°çš„ hop size ä¼šå¯¼è‡´å¸§ä¹‹é—´æœ‰æ›´å¤šçš„é‡å ï¼Œä»è€Œæé«˜æ—¶é—´åˆ†è¾¨ç‡ï¼Œä½†è®¡ç®—å¼€é”€æ›´å¤§ã€‚
    - å¤§çš„ hop size ä¼šå‡å°‘å¸§ä¹‹é—´çš„é‡å ï¼Œä»è€Œé™ä½æ—¶é—´åˆ†è¾¨ç‡ï¼Œä½†è®¡ç®—æ›´å¿«ã€‚

### ä¸¾ä¾‹è¯´æ˜

å‡è®¾ä»¥ä¸‹å‚æ•°ï¼š

- **é‡‡æ ·ç‡**: 16 kHzã€‚
- **çª—å£å¤§å° (window size)**: 25 ms ï¼ˆå¯¹åº” 400 ä¸ªæ ·æœ¬ï¼‰ã€‚
- **hop size**: 10 ms ï¼ˆå¯¹åº” 160 ä¸ªæ ·æœ¬ï¼‰ã€‚

éŸ³é¢‘å¤„ç†æ­¥éª¤ï¼š

1. ç¬¬ä¸€ä¸ªçª—å£ä» 0 åˆ° 400 ä¸ªæ ·æœ¬ã€‚
2. ä¸‹ä¸€ä¸ªçª—å£ä» 160 åˆ° 560 ä¸ªæ ·æœ¬ï¼ˆæ»‘åŠ¨ 160 ä¸ªæ ·æœ¬ï¼Œç›¸å½“äº strideï¼‰ã€‚
3. é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¦†ç›–æ•´ä¸ªéŸ³é¢‘ã€‚

å¦‚æœ hop size ä¸º 160ï¼ˆ10 msï¼‰ï¼Œå¸§ä¹‹é—´æœ‰ **240 ä¸ªæ ·æœ¬ï¼ˆ15 msï¼‰é‡å **ã€‚

### æ€»ç»“

- **hop size â‰ˆ stride**ï¼Œå®ƒå†³å®šçª—å£åœ¨éŸ³é¢‘æ•°æ®ä¸Šçš„æ»‘åŠ¨æ­¥é•¿ã€‚
- å°çš„ hop size ç›¸å½“äºæ›´ç»†çš„æ—¶é—´åˆ†è¾¨ç‡ï¼ˆæ›´å¤šå¸§ã€æ›´å¤šè®¡ç®—ï¼‰ã€‚
- å¤§çš„ hop size ç›¸å½“äºæ›´ç²—çš„æ—¶é—´åˆ†è¾¨ç‡ï¼ˆæ›´å°‘å¸§ã€è®¡ç®—æ›´å¿«ï¼‰ã€‚

å¦‚æœéœ€è¦æ›´æ·±å…¥çš„ç†è§£æˆ–ä»£ç å®ç°ç¤ºä¾‹ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼



### **Whisper çš„ Attention å±¬æ–¼å“ªä¸€ç¨®é¡å‹ï¼Ÿ**

Whisper æ˜¯ OpenAI é–‹ç™¼çš„ä¸€ç¨® **èªéŸ³è­˜åˆ¥æ¨¡å‹ï¼ˆASR, Automatic Speech Recognitionï¼‰**ï¼ŒåŸºæ–¼ **Transformer æ¶æ§‹**ï¼Œå› æ­¤å®ƒçš„ **Attention æ©Ÿåˆ¶** ä¾†è‡ªæ–¼ Transformer çš„è¨­è¨ˆã€‚æ ¹æ“šå…¶æ¶æ§‹ï¼ŒWhisper çš„ Attention ä¸»è¦åˆ†ç‚ºä»¥ä¸‹å¹¾ç¨®é¡å‹ï¼š

---

## **1. Whisper çš„ Attention æ¦‚è¦½**

Whisper æ¡ç”¨äº†æ¨™æº–çš„ **Encoder-Decoder Transformer æ¶æ§‹**ï¼Œå› æ­¤å®ƒçš„ Attention é¡å‹åŒ…æ‹¬ï¼š

1. **Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼šç”¨æ–¼ Encoder å’Œ Decoder å…§éƒ¨ï¼Œè®“è©æˆ–éŸ³è¨Šç‰‡æ®µå½¼æ­¤é—œè¯ã€‚
2. **Masked Self-Attentionï¼ˆæ©ç¢¼è‡ªæ³¨æ„åŠ›ï¼‰**ï¼šç”¨æ–¼ Decoderï¼Œç¢ºä¿è§£ç¢¼æ™‚ä¸æœƒæå‰çœ‹åˆ°æœªä¾†çš„è©ï¼ˆé¡ä¼¼ GPTï¼‰ã€‚
3. **Encoder-Decoder Attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰**ï¼šè®“ Decoder å°‡ **éŸ³è¨Šç·¨ç¢¼çµæœ**ï¼ˆEncoder è¼¸å‡ºï¼‰èˆ‡ **èªè¨€ç”Ÿæˆ**ï¼ˆDecoderï¼‰é—œè¯èµ·ä¾†ã€‚

### **ğŸ“Œ Whisper çš„ Attention vs. ä¸€èˆ¬ Transformer**

|**æ¨¡å‹éƒ¨åˆ†**|**Attention é¡å‹**|**ä½œç”¨**|
|---|---|---|
|**Encoder**ï¼ˆéŸ³è¨Šè™•ç†ï¼‰|**Self-Attention**|è®“éŸ³è¨Šç‰‡æ®µç›¸äº’é—œè¯ï¼Œæå–å…¨å±€è³‡è¨Š|
|**Decoder**ï¼ˆèªè¨€ç”Ÿæˆï¼‰|**Masked Self-Attention**|ç¢ºä¿ç”Ÿæˆæ™‚åªè€ƒæ…®å·²ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œé˜²æ­¢è³‡è¨Šæ´©æ¼|
|**Decoder**ï¼ˆèˆ‡ Encoder äº¤äº’ï¼‰|**Encoder-Decoder Attention**|è®“èªéŸ³çš„ç‰¹å¾µèˆ‡æ–‡å­—åºåˆ—å°æ‡‰ï¼Œç¢ºä¿æ­£ç¢ºè½‰éŒ„|

---

## **2. Whisper çš„ Attention è©³ç´°è§£æ**

### **(1) Encoder ä½¿ç”¨ Self-Attention**

**ğŸ“Œ Whisper çš„ Encoder è² è²¬è™•ç†éŸ³è¨Šæ•¸æ“šï¼Œå°‡è²éŸ³è½‰æ›ç‚ºéš±è—è¡¨ç¤ºï¼ˆHidden Representationsï¼‰**

- Self-Attention è®“æ¨¡å‹èƒ½å¤ è€ƒæ…® **æ•´æ®µéŸ³è¨Šçš„ä¸Šä¸‹æ–‡è³‡è¨Š**ï¼Œæ•æ‰èªéŸ³ç‰¹å¾µçš„é•·è·é›¢ä¾è³´é—œä¿‚ã€‚
- é€™èˆ‡ BERT çš„ **é›™å‘ï¼ˆBidirectionalï¼‰Self-Attention** é¡ä¼¼ï¼Œæ¯å€‹éŸ³è¨Š token éƒ½èƒ½çœ‹åˆ°æ•´å€‹éŸ³è¨Šç‰‡æ®µï¼Œå¾è€Œå­¸ç¿’æœ€ä½³çš„è¡¨å¾µã€‚

**ğŸ¯ é¡æ¯” Transformer çš„ BERT Encoderï¼š**

AttentionÂ Score=QKTdk\text{Attention Score} = \frac{Q K^T}{\sqrt{d_k}}

é€™å…è¨±æ¨¡å‹ç¢ºå®šæŸå€‹éŸ³ç´ ï¼ˆPhonemeï¼‰æˆ–è©çš„ç™¼éŸ³èˆ‡å…¶ä»–éƒ¨åˆ†çš„é—œè¯ï¼Œä¾‹å¦‚ï¼š

- ã€Œhello worldã€çš„ã€Œhã€å¯èƒ½æœƒå½±éŸ¿ã€Œoã€çš„ç™¼éŸ³ã€‚

---

### **(2) Decoder ä½¿ç”¨ Masked Self-Attention**

**ğŸ“Œ Whisper çš„ Decoder è² è²¬å°‡ç·¨ç¢¼éçš„éŸ³è¨Šè½‰æ›æˆæ–‡å­—**

- Masked Self-Attention ç¢ºä¿ Decoder åœ¨ç”Ÿæˆè©èªæ™‚ï¼Œä¸æœƒæå‰çœ‹åˆ°æœªä¾†çš„è©ï¼Œé€™èˆ‡ GPT çš„è¡Œç‚ºç›¸åŒã€‚
- é€™ç¢ºä¿äº†æ¨¡å‹åœ¨æ¨ç†æ™‚èƒ½å¤  **è‡ªå›æ­¸ï¼ˆAuto-Regressiveï¼‰åœ°é€æ­¥ç”¢ç”Ÿè¼¸å‡º**ã€‚

**ğŸ¯ èˆ‰ä¾‹ï¼š** åœ¨è½‰éŒ„ `"hello world"` æ™‚ï¼š

- ç”¢ç”Ÿ `"hello"` æ™‚ï¼Œä¸èƒ½æå‰çŸ¥é“ `"world"`ï¼Œåªèƒ½æ ¹æ“š `"hello"` ä¾†é æ¸¬ä¸‹ä¸€å€‹è©ã€‚

é€™é€é **ä¸‹ä¸‰è§’æ©ç¢¼ï¼ˆMaskingï¼‰** ä¾†å¯¦ç¾ï¼š

MaskedÂ AttentionÂ Score=[âœ”000âœ”âœ”00âœ”âœ”âœ”0âœ”âœ”âœ”âœ”]\text{Masked Attention Score} = \begin{bmatrix} \text{âœ”} & \text{0} & \text{0} & \text{0} \\ \text{âœ”} & \text{âœ”} & \text{0} & \text{0} \\ \text{âœ”} & \text{âœ”} & \text{âœ”} & \text{0} \\ \text{âœ”} & \text{âœ”} & \text{âœ”} & \text{âœ”} \end{bmatrix}

é€™æ¨£ç¢ºä¿ï¼š

- **åªèƒ½è€ƒæ…®éå»è©**ï¼Œè€Œä¸èƒ½çœ‹åˆ°æœªä¾†è©ï¼ˆé˜²æ­¢æ´©æ¼è³‡è¨Šï¼‰ã€‚
- é€™èˆ‡ GPTã€ChatGPT ç­‰èªè¨€æ¨¡å‹ç›¸ä¼¼ã€‚

---

### **(3) Encoder-Decoder Attention**

**ğŸ“Œ Whisper çš„ Decoder éœ€è¦ç†è§£éŸ³è¨Šå…§å®¹ï¼Œå› æ­¤ä½¿ç”¨ Encoder-Decoder Attention**

- é€™æ˜¯ Transformer **ç¿»è­¯æ¨¡å‹**ï¼ˆå¦‚ T5, BARTï¼‰ä¸­æ¨™æº–çš„ Attention æ©Ÿåˆ¶ã€‚
- Decoder çš„ Queryï¼ˆQï¼‰ä¾†è‡ª **å·²ç¶“ç”Ÿæˆçš„è©**ï¼Œè€Œ Keyï¼ˆKï¼‰å’Œ Valueï¼ˆVï¼‰ä¾†è‡ª **Encoder è¼¸å‡ºçš„éŸ³è¨Šè¡¨ç¤º**ã€‚
- é€™å…è¨±æ¨¡å‹å°‡ **èªéŸ³è³‡è¨Šèˆ‡æ–‡å­—å…§å®¹å°é½Š**ï¼Œä»¥ç¢ºä¿æº–ç¢ºçš„è½‰éŒ„ã€‚

**ğŸ¯ èˆ‰ä¾‹** ç•¶è½‰éŒ„ `"hello world"`ï¼š

- `"hello"` åœ¨ Decoder ä¸­ä½œç‚º Queryï¼ˆQï¼‰ï¼Œèˆ‡ Encoder æä¾›çš„éŸ³è¨Šè¡¨å¾µé€²è¡ŒåŒ¹é…ï¼Œç¢ºä¿ `"hello"` ä¾†è‡ª `"hÉ›loÊŠ"`ã€‚
- é€™ç¢ºä¿ Whisper ä¸åƒ…èƒ½è­˜åˆ¥å–®è©ï¼Œé‚„èƒ½å°æ‡‰ **éŸ³è¨Šèˆ‡èªç¾©çš„é—œä¿‚**ã€‚

é€™èˆ‡ **æ©Ÿå™¨ç¿»è­¯ï¼ˆMachine Translationï¼‰** çš„ Attention ä¸€æ¨£ï¼Œè®“ Decoder é¸æ“‡é©ç•¶çš„éŸ³è¨Šç‰‡æ®µä¾†ç”¢ç”Ÿå°æ‡‰çš„è©ã€‚

---

## **3. Whisper Attention çš„é—œéµç‰¹é»**

|**Attention é¡å‹**|**Whisper çš„æ‡‰ç”¨**|**èˆ‡æ¨™æº– Transformer çš„å€åˆ¥**|
|---|---|---|
|**Self-Attention (Encoder)**|åˆ†ææ•´æ®µéŸ³è¨Šç‰¹å¾µ|èˆ‡ BERT é¡ä¼¼ï¼Œé›™å‘å­¸ç¿’èªéŸ³è¡¨å¾µ|
|**Masked Self-Attention (Decoder)**|é¿å…æœªä¾†è³‡è¨Šæ´©æ¼|èˆ‡ GPT é¡ä¼¼ï¼Œç¢ºä¿é€æ­¥ç”Ÿæˆ|
|**Encoder-Decoder Attention**|è®“ Decoder è®€å– Encoder æä¾›çš„éŸ³è¨Šè³‡è¨Š|èˆ‡æ©Ÿå™¨ç¿»è­¯é¡ä¼¼ï¼Œå°æ‡‰éŸ³è¨Šèˆ‡æ–‡å­—|

### ğŸš€ Whisper çš„ Attention ç‰¹é»ï¼š

âœ… **é›™å‘éŸ³è¨Šè™•ç†ï¼ˆSelf-Attention in Encoderï¼‰**ï¼šèƒ½å¤ æ•æ‰èªéŸ³çš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œæé«˜è½‰éŒ„æº–ç¢ºæ€§ã€‚  
âœ… **è‡ªå›æ­¸èªè¨€ç”Ÿæˆï¼ˆMasked Self-Attention in Decoderï¼‰**ï¼šç¢ºä¿é€æ­¥è§£ç¢¼ï¼Œä¸æœƒæå‰çŸ¥é“æœªä¾†è©ã€‚  
âœ… **è·¨æ¨¡æ…‹æ˜ å°„ï¼ˆEncoder-Decoder Attentionï¼‰**ï¼šå…è¨±èªéŸ³ç‰¹å¾µèˆ‡èªè¨€ç‰¹å¾µå°é½Šï¼Œæå‡ ASR è½‰éŒ„æº–ç¢ºæ€§ã€‚

---

## **4. Whisper Attention vs. å…¶ä»–æ¨¡å‹**

|**æ¨¡å‹**|**Attention é¡å‹**|**æ‡‰ç”¨å ´æ™¯**|
|---|---|---|
|**GPT-3 / ChatGPT**|Decoder-Only, Masked Self-Attention|ç´”æ–‡å­—ç”Ÿæˆï¼ˆNLPï¼‰|
|**BERT**|Encoder-Only, Self-Attention|èªè¨€ç†è§£ï¼ˆNLPï¼‰|
|**Whisper**|Encoder-Decoder, Self-Attention + Masked + Cross|èªéŸ³è½‰éŒ„ï¼ˆASRï¼‰|
|**T5 / BART**|Encoder-Decoder, Self-Attention + Cross|ç¿»è­¯ã€æ‘˜è¦|

---

## **5. çµè«–**

Whisper æ¡ç”¨ **æ¨™æº–çš„ Transformer Encoder-Decoder æ¶æ§‹**ï¼Œå› æ­¤åŒ…å«ï¼š

4. **Self-Attentionï¼ˆEncoderï¼‰** â†’ åˆ†æèªéŸ³ç‰¹å¾µ
5. **Masked Self-Attentionï¼ˆDecoderï¼‰** â†’ é€æ­¥ç”¢ç”Ÿè¼¸å‡ºè©
6. **Encoder-Decoder Attention** â†’ è®“éŸ³è¨Šç‰¹å¾µèˆ‡èªè¨€è½‰éŒ„å°é½Š

ğŸš€ **Whisper çš„ Attention è¨­è¨ˆé¡ä¼¼æ–¼æ©Ÿå™¨ç¿»è­¯ï¼ˆå¦‚ T5ã€BARTï¼‰ï¼Œä½†é©ç”¨æ–¼èªéŸ³è½‰éŒ„ï¼Œä½¿å…¶èƒ½å¤ é«˜æ•ˆåœ°å°‡éŸ³è¨Šè½‰æ›ç‚ºæ–‡å­—ï¼** ğŸ¯


[Whisper Timestamps Â· Issue #647 Â· NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/issues/647)